{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# 1) Configuraci\u00f3n\n**Libreta auto-generada \u2014 SARIMAX \u00fanicamente (Yahoo Finance, descarga por ventanas, Optuna, m\u00e9tricas SOTA)**"}, {"cell_type": "code", "metadata": {}, "source": "# --- Instalaci\u00f3n opcional de dependencias (si faltan). Desactiva si prefieres manejarlo t\u00fa. ---\nALLOW_PIP = True\ndef _ensure(pkg, pip_name=None):\n    try:\n        __import__(pkg)\n    except Exception as e:\n        if ALLOW_PIP:\n            import sys, subprocess\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pip_name or pkg])\n        else:\n            print(f\"[WARN] Falta {pkg}. Inst\u00e1lalo manualmente.\")\nfor pkg, pipn in [\n    (\"yfinance\",\"yfinance\"),\n    (\"pandas\",\"pandas\"),\n    (\"numpy\",\"numpy\"),\n    (\"statsmodels\",\"statsmodels\"),\n    (\"matplotlib\",\"matplotlib\"),\n    (\"tqdm\",\"tqdm\"),\n    (\"pandas_market_calendars\",\"pandas-market-calendars\"),\n    (\"holidays\",\"holidays\"),\n    (\"optuna\",\"optuna\")\n]:\n    _ensure(pkg, pipn)\n\nimport os, math, json, warnings, time, traceback\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport yfinance as yf\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\n\nimport optuna\nfrom optuna.pruners import MedianPruner\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.graphics.gofplots import qqplot\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\nfrom statsmodels.stats.stattools import jarque_bera\n\nimport pandas_market_calendars as mcal\n\n# --- Par\u00e1metros editables ---\nTICKER = \"BTC-USD\"\nINTERVAL = \"1h\"       # \"1h\",\"30m\",\"15m\",\"1d\", etc.\nSTART = None          # \"2015-01-01\" o None\nEND = None            # \"2025-09-08\" o None\nH = 48                # horizonte de pron\u00f3stico en pasos de INTERVAL\nREMOVE_HOLIDAYS = False\nMARKET_CAL = \"XNYS\"   # Calendario para limpieza de feriados/horas\nTZ = \"UTC\"            # Zona horaria destino\nIMPUTE_MISSING = True\nUSE_EXOG = True\n\n# \u00d3rdenes opcionales (si None, se optimizan con Optuna)\nORDER = None           # p,d,q  e.g., (1,1,1) o None\nSEASONAL_ORDER = None  # P,D,Q,m e.g., (0,1,1,24) o None\n\nMETRIC_OPT = \"sMAPE\"   # {\"RMSE\",\"MAE\",\"sMAPE\",\"MASE\"}\nBACKTEST_FOLDS = 3     # 0 para desactivar\nN_TRIALS = 60\nTIMEOUT_MIN = 20\nN_JOBS = 1\nPRUNING = True\nRANDOM_SEED = 42\n\n# Carpeta de salida\nOUT_DIR = \"./outputs\"\nos.makedirs(OUT_DIR, exist_ok=True)\n\nprint(\"Par\u00e1metros activos:\")\nprint(dict(TICKER=TICKER, INTERVAL=INTERVAL, START=START, END=END, H=H,\n           REMOVE_HOLIDAYS=REMOVE_HOLIDAYS, MARKET_CAL=MARKET_CAL, TZ=TZ,\n           IMPUTE_MISSING=IMPUTE_MISSING, USE_EXOG=USE_EXOG,\n           ORDER=ORDER, SEASONAL_ORDER=SEASONAL_ORDER,\n           METRIC_OPT=METRIC_OPT, BACKTEST_FOLDS=BACKTEST_FOLDS,\n           N_TRIALS=N_TRIALS, TIMEOUT_MIN=TIMEOUT_MIN, N_JOBS=N_JOBS, PRUNING=PRUNING,\n           RANDOM_SEED=RANDOM_SEED))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 2) Descarga ventana-a-ventana (Yahoo Finance)"}, {"cell_type": "code", "metadata": {}, "source": "# Helpers de rango permitido por INTERVAL en Yahoo (aprox. seguro) y tama\u00f1o de ventana\ndef _allowed_days_and_step(interval: str):\n    interval = interval.lower()\n    if interval in (\"1m\",\"2m\"):\n        return 30, 5\n    if interval in (\"5m\",\"15m\",\"30m\"):\n        return 60, 10\n    if interval in (\"60m\",\"1h\",\"90m\"):\n        return 730, 60\n    if interval in (\"1d\",\"5d\",\"1wk\"):\n        return 10000, 365*2\n    return 3650, 180\n\ndef _to_pd_freq(interval: str) -> str:\n    m = interval.lower()\n    if m.endswith(\"m\"):\n        return f\"{m[:-1]}min\"\n    if m.endswith(\"h\"):\n        return f\"{m[:-1]}H\"\n    if m.endswith(\"d\"):\n        return \"D\"\n    if m.endswith(\"wk\"):\n        return \"W\"\n    return \"D\"\n\ndef download_yf_windowed(ticker: str, interval: str, start=None, end=None, want_max=True, tz=\"UTC\", max_retries=3, sleep=1.0):\n    allowed_days, step_days = _allowed_days_and_step(interval)\n    if end is None:\n        end = pd.Timestamp.utcnow().tz_localize(\"UTC\")\n    else:\n        end = pd.Timestamp(end)\n        if end.tzinfo is None: end = end.tz_localize(\"UTC\")\n        else: end = end.tz_convert(\"UTC\")\n    if start is None:\n        if want_max:\n            # Pedimos muy atr\u00e1s; Yahoo recortar\u00e1 a lo disponible.\n            start = end - pd.Timedelta(days=allowed_days*50)\n        else:\n            start = end - pd.Timedelta(days=allowed_days)\n    else:\n        start = pd.Timestamp(start)\n        if start.tzinfo is None: start = start.tz_localize(\"UTC\")\n        else: start = start.tz_convert(\"UTC\")\n\n    dfs = []\n    cur = start\n    pbar = tqdm(total=None, desc=f\"Descargando {ticker} {interval} por ventanas\")\n    while cur < end:\n        win_end = min(cur + pd.Timedelta(days=step_days), end)\n        retries = 0\n        got = None\n        while retries <= max_retries:\n            try:\n                df = yf.download(ticker, interval=interval, start=cur, end=win_end, progress=False, auto_adjust=False, prepost=False, threads=True)\n                if isinstance(df, pd.DataFrame) and not df.empty:\n                    got = df.copy()\n                break\n            except Exception as e:\n                retries += 1\n                time.sleep(sleep * (2**retries))\n        if got is not None and not got.empty:\n            got = got.rename(columns=str.lower)\n            if \"adj close\" in got.columns and \"close\" not in got.columns and \"adj_close\" not in got.columns:\n                got = got.rename(columns={\"adj close\": \"close\"})\n            dfs.append(got)\n        cur = win_end\n        pbar.update(1)\n    pbar.close()\n\n    if len(dfs) == 0:\n        raise RuntimeError(\"No se descargaron datos (posible combinaci\u00f3n inv\u00e1lida ticker/intervalo).\")\n    data = pd.concat(dfs).sort_index()\n    data = data.loc[~data.index.duplicated(keep=\"last\")]\n    # Hacer \u00edndice tz-aware\n    if data.index.tz is None:\n        data.index = data.index.tz_localize(\"UTC\")\n    else:\n        data.index = data.index.tz_convert(\"UTC\")\n    data = data.tz_convert(tz)\n    # Mantener columnas OHLCV\n    cols = [c for c in [\"open\",\"high\",\"low\",\"close\",\"volume\"] if c in data.columns]\n    data = data[cols].astype(float)\n    return data\n\nraw = download_yf_windowed(TICKER, INTERVAL, START, END, want_max=True, tz=TZ)\nprint(raw.tail(3))\nprint(raw.index[0], \"->\", raw.index[-1], f\"({len(raw)} filas)\")", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 3) Limpieza / Calendario (frecuencia fija, imputaci\u00f3n, feriados)"}, {"cell_type": "code", "metadata": {}, "source": "def fix_frequency(df: pd.DataFrame, interval: str, impute=True):\n    freq = _to_pd_freq(interval)\n    # Reindex a rejilla completa\n    full_idx = pd.date_range(df.index.min(), df.index.max(), freq=freq, tz=df.index.tz)\n    out = df.reindex(full_idx)\n    # Imputaci\u00f3n simple\n    if impute:\n        out[\"close\"] = out[\"close\"].ffill()\n        for c in [\"open\",\"high\",\"low\",\"volume\"]:\n            if c in out.columns:\n                if c == \"volume\":\n                    out[c] = out[c].fillna(0.0)\n                else:\n                    out[c] = out[c].fillna(method=\"ffill\").fillna(out[\"close\"])\n    return out\n\ndef filter_market_sessions(df: pd.DataFrame, interval: str, cal_name=\"XNYS\"):\n    # Solo intrad\u00eda\n    if interval.lower().endswith((\"m\",\"h\")):\n        cal = mcal.get_calendar(cal_name)\n        freq = _to_pd_freq(interval)\n        sched = cal.schedule(start_date=df.index.min().date() - pd.Timedelta(days=7),\n                             end_date=df.index.max().date() + pd.Timedelta(days=7))\n        # Genera rango de sesiones con la misma frecuencia\n        trade_idx = mcal.date_range(sched, frequency=freq, force_close=True)\n        # Asegura misma zona\n        trade_idx = trade_idx.tz_convert(df.index.tz)\n        # Reindex para remover fuera de sesi\u00f3n/feriados\n        df2 = df.reindex(trade_idx)\n        return df2\n    return df\n\ndata = fix_frequency(raw, INTERVAL, impute=IMPUTE_MISSING)\nif REMOVE_HOLIDAYS:\n    data = filter_market_sessions(data, INTERVAL, MARKET_CAL)\n\nprint(data.tail(3))", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 4) Split + (opcional) Backtesting (rolling origin)"}, {"cell_type": "code", "metadata": {}, "source": "def make_exog(idx: pd.DatetimeIndex):\n    # Dummies de hora/d\u00eda (sin colinealidad)\n    df = pd.DataFrame(index=idx)\n    if any(k in INTERVAL for k in [\"m\",\"h\"]):\n        df[\"hour\"] = idx.hour\n        ex_hour = pd.get_dummies(df[\"hour\"], prefix=\"hr\", drop_first=True, dtype=float)\n    else:\n        ex_hour = pd.DataFrame(index=idx)\n    df[\"dow\"] = idx.dayofweek\n    ex_dow = pd.get_dummies(df[\"dow\"], prefix=\"dow\", drop_first=True, dtype=float)\n    ex = pd.concat([ex_hour, ex_dow], axis=1)\n    if ex.shape[1] == 0:\n        ex[\"bias\"] = 1.0\n    return ex\n\ndef simple_split(y: pd.Series, test_h: int, val_frac=0.15):\n    n = len(y)\n    test_start = n - test_h\n    val_end = test_start\n    val_start = max(0, int(val_end - n*val_frac))\n    train = y.iloc[:val_start]\n    val = y.iloc[val_start:val_end]\n    test = y.iloc[test_start:]\n    return train, val, test\n\ndef rolling_origin_splits(y: pd.Series, folds: int, val_len: int):\n    # Ventanas crecientes, validaci\u00f3n fija val_len\n    starts = np.linspace(len(y)*0.5, len(y)-val_len*1.5, num=folds).astype(int)\n    for s in starts:\n        tr = y.iloc[:s]\n        va = y.iloc[s:s+val_len]\n        yield tr, va\n\ny = data[\"close\"].astype(float).copy()\ntrain, val, test = simple_split(y, test_h=H, val_frac=0.15)\n\nEXOG_ON_DEFAULT = USE_EXOG\nexog_all = make_exog(y.index) if USE_EXOG else None\nexog_train = exog_all.loc[train.index] if exog_all is not None else None\nexog_val = exog_all.loc[val.index] if exog_all is not None else None\nexog_test = exog_all.loc[test.index] if exog_all is not None else None\n\nprint(f\"Tama\u00f1os -> train:{len(train)} val:{len(val)} test:{len(test)} | \u00daltimo \u00edndice train:{train.index[-1]}\")", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 5) Hypertuning con Optuna (minimiza m\u00e9trica en validaci\u00f3n o promedio de folds)"}, {"cell_type": "code", "metadata": {}, "source": "rng = np.random.default_rng(RANDOM_SEED)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\ndef smape(y_true, y_pred, eps=1e-8):\n    a = np.abs(y_pred - y_true)\n    b = (np.abs(y_true) + np.abs(y_pred)) / 2.0 + eps\n    return float(np.mean(a / b))\n\ndef mase(y_true, y_pred, y_train, m=1):\n    # naive lag-1\n    if len(y_train) <= 1:\n        return np.nan\n    naive_diff = np.abs(np.diff(y_train, n=m)).mean()\n    return float(np.mean(np.abs(y_true - y_pred)) / (naive_diff + 1e-8))\n\ndef metric_value(name, y_true, y_pred, y_train=None):\n    name = name.upper()\n    if name == \"RMSE\":\n        return float(np.sqrt(np.mean((y_true - y_pred)**2)))\n    if name == \"MAE\":\n        return float(np.mean(np.abs(y_true - y_pred)))\n    if name == \"SMAPE\":\n        return smape(y_true, y_pred)\n    if name == \"MASE\":\n        return mase(y_true, y_pred, y_train if y_train is not None else y_true, m=1)\n    return smape(y_true, y_pred)\n\ndef seasonal_candidates(interval: str):\n    m = interval.lower()\n    if m == \"1h\":\n        return [24, 24*7]\n    if m == \"30m\":\n        return [48, 48*7]\n    if m == \"15m\":\n        return [96, 96*7]\n    if m == \"1d\":\n        return [7, 30, 365]\n    return [0, 7]\n\ndef build_and_score(y_tr, y_va, ex_tr, ex_va, params):\n    try:\n        mod = SARIMAX(\n            y_tr, exog=ex_tr,\n            order=tuple(params[\"order\"]),\n            seasonal_order=tuple(params[\"seasonal_order\"]),\n            trend=params[\"trend\"],\n            enforce_stationarity=params[\"enforce_stationarity\"],\n            enforce_invertibility=params[\"enforce_invertibility\"]\n        )\n        res = mod.fit(disp=False)\n        fc = res.get_forecast(steps=len(y_va), exog=ex_va)\n        pred = pd.Series(fc.predicted_mean, index=y_va.index).astype(float)\n        val = metric_value(METRIC_OPT, y_va.values, pred.values, y_tr.values)\n        return val, res\n    except Exception as e:\n        return 1e12, None\n\ndef objective(trial: optuna.Trial):\n    p = trial.suggest_int(\"p\", 0, 5)\n    d = trial.suggest_int(\"d\", 0, 2)\n    q = trial.suggest_int(\"q\", 0, 5)\n    seas_m_list = seasonal_candidates(INTERVAL)\n    if len(seas_m_list) and max(seas_m_list) > 0:\n        P = trial.suggest_int(\"P\", 0, 2)\n        D = trial.suggest_int(\"D\", 0, 1)\n        Q = trial.suggest_int(\"Q\", 0, 2)\n        m = trial.suggest_categorical(\"m\", seas_m_list)\n    else:\n        P = D = Q = 0\n        m = 0\n    trend = trial.suggest_categorical(\"trend\", [\"n\",\"c\",\"t\",\"ct\"])\n    ex_on = trial.suggest_categorical(\"EXOG_ON\", [1, 0]) if USE_EXOG else 0\n    enforce_stat = trial.suggest_categorical(\"enforce_stationarity\", [True, False])\n    enforce_inv = trial.suggest_categorical(\"enforce_invertibility\", [True, False])\n\n    params = dict(\n        order=(p,d,q),\n        seasonal_order=(P,D,Q,m),\n        trend=trend,\n        enforce_stationarity=enforce_stat,\n        enforce_invertibility=enforce_inv\n    )\n    if BACKTEST_FOLDS and BACKTEST_FOLDS > 0:\n        val_len = max(H, max(16, int(len(y)*0.1)))\n        scores = []\n        for i, (y_tr, y_va) in enumerate(rolling_origin_splits(y, BACKTEST_FOLDS, val_len)):\n            ex_tr = make_exog(y_tr.index) if (USE_EXOG and ex_on==1) else None\n            ex_va = make_exog(y_va.index) if (USE_EXOG and ex_on==1) else None\n            sc, _ = build_and_score(y_tr, y_va, ex_tr, ex_va, params)\n            trial.report(sc, step=i)\n            if PRUNING and trial.should_prune():\n                raise optuna.TrialPruned()\n            scores.append(sc)\n        return float(np.mean(scores))\n    else:\n        ex_tr = make_exog(train.index) if (USE_EXOG and ex_on==1) else None\n        ex_va = make_exog(val.index) if (USE_EXOG and ex_on==1) else None\n        sc, _ = build_and_score(train, val, ex_tr, ex_va, params)\n        return sc\n\nsampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\npruner = MedianPruner(n_startup_trials=min(10, N_TRIALS//4)) if PRUNING else optuna.pruners.NopPruner()\nstudy = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\nstudy.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT_MIN*60, n_jobs=N_JOBS, show_progress_bar=True)\n\nbest_params = study.best_trial.params\nprint(\"Mejor m\u00e9trica (\", METRIC_OPT, \"):\", study.best_value)\nprint(\"Mejores params:\", best_params)\n\n# Guardar resultados Optuna\nwith open(os.path.join(OUT_DIR, \"best_params.json\"), \"w\") as f:\n    json.dump(best_params, f, indent=2)\n\n# Historial\ndf_trials = study.trials_dataframe()\ndf_trials.to_csv(os.path.join(OUT_DIR, \"trials.csv\"), index=False)\n\n# Plots opcionales\ntry:\n    fig1 = optuna.visualization.plot_optimization_history(study)\n    fig1.write_image(os.path.join(OUT_DIR, \"opt_history.png\"))\nexcept Exception as e:\n    pass\ntry:\n    fig2 = optuna.visualization.plot_param_importances(study)\n    fig2.write_image(os.path.join(OUT_DIR, \"opt_importances.png\"))\nexcept Exception as e:\n    pass\ntry:\n    fig3 = optuna.visualization.plot_slice(study)\n    fig3.write_image(os.path.join(OUT_DIR, \"opt_slice.png\"))\nexcept Exception as e:\n    pass", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 6) Entrenamiento final con mejores hiperpar\u00e1metros"}, {"cell_type": "code", "metadata": {}, "source": "def params_from_best(best: dict):\n    p = best.get(\"p\", 1)\n    d = best.get(\"d\", 1)\n    q = best.get(\"q\", 1)\n    P = best.get(\"P\", 0)\n    D = best.get(\"D\", 0)\n    Q = best.get(\"Q\", 0)\n    m = best.get(\"m\", 0)\n    trend = best.get(\"trend\", \"c\")\n    enforce_stationarity = best.get(\"enforce_stationarity\", True)\n    enforce_invertibility = best.get(\"enforce_invertibility\", True)\n    return dict(order=(p,d,q), seasonal_order=(P,D,Q,m), trend=trend,\n                enforce_stationarity=enforce_stationarity, enforce_invertibility=enforce_invertibility)\n\nfinal_params = params_from_best(best_params) if ORDER is None or SEASONAL_ORDER is None else dict(order=ORDER, seasonal_order=SEASONAL_ORDER, trend=best_params.get(\"trend\",\"c\"), enforce_stationarity=True, enforce_invertibility=True)\n\n# Entrenar con TRAIN+VAL para aprovechar m\u00e1s datos\ny_fit = pd.concat([train, val])\nex_fit = make_exog(y_fit.index) if USE_EXOG and best_params.get(\"EXOG_ON\", 1)==1 else (make_exog(y_fit.index) if USE_EXOG and (ORDER is not None or SEASONAL_ORDER is not None) else None)\n\nmodel = SARIMAX(y_fit, exog=ex_fit, **final_params)\nres = model.fit(disp=False)\nsummary_text = str(res.summary())\nprint(summary_text.splitlines()[:15])\n\n# Guardar resumen AIC/BIC\nwith open(os.path.join(OUT_DIR, \"summary.txt\"), \"w\", encoding=\"utf-8\") as f:\n    f.write(summary_text)\n    f.write(\"\\n\\nAIC=\" + str(res.aic) + \"  BIC=\" + str(res.bic))\nprint(\"AIC:\", res.aic, \"BIC:\", res.bic)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 7) Pron\u00f3stico + intervalos (80% y 95%)"}, {"cell_type": "code", "metadata": {}, "source": "# Pron\u00f3stico sobre el conjunto de TEST (tama\u00f1o H)\nex_future = make_exog(test.index) if USE_EXOG and (best_params.get(\"EXOG_ON\", 1)==1 or (ORDER is not None or SEASONAL_ORDER is not None)) else None\nfc = res.get_forecast(steps=len(test), exog=ex_future)\npred = pd.Series(fc.predicted_mean, index=test.index, name=\"y_pred\").astype(float)\nci80 = fc.conf_int(alpha=0.2);  ci95 = fc.conf_int(alpha=0.05)\nlower80 = pd.Series(ci80.iloc[:,0].values, index=test.index, name=\"lower80\")\nupper80 = pd.Series(ci80.iloc[:,1].values, index=test.index, name=\"upper80\")\nlower95 = pd.Series(ci95.iloc[:,0].values, index=test.index, name=\"lower95\")\nupper95 = pd.Series(ci95.iloc[:,1].values, index=test.index, name=\"upper95\")\n\nforecast_df = pd.concat([test.rename(\"y_true\"), pred, lower80, upper80, lower95, upper95], axis=1)\nforecast_df.to_csv(os.path.join(OUT_DIR, \"forecast.csv\"))\nforecast_df.tail(3)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 8) M\u00e9tricas (MAE, RMSE, MAPE, sMAPE, MASE) + Coberturas"}, {"cell_type": "code", "metadata": {}, "source": "def mape(y_true, y_pred, eps=1e-8):\n    return float(np.mean(np.abs((y_true - y_pred) / (y_true + eps))) * 100.0)\n\ndef coverage(y_true, lo, up):\n    inside = ((y_true >= lo) & (y_true <= up)).mean()\n    return float(inside)\n\ny_true = forecast_df[\"y_true\"].values\ny_pred = forecast_df[\"y_pred\"].values\n\nmetrics = {\n    \"MAE\": float(np.mean(np.abs(y_true - y_pred))),\n    \"RMSE\": float(np.sqrt(np.mean((y_true - y_pred)**2))),\n    \"MAPE\": mape(y_true, y_pred),\n    \"sMAPE\": smape(y_true, y_pred),\n    \"MASE\": mase(y_true, y_pred, y_fit.values, m=1),\n    \"COV80\": coverage(y_true, forecast_df[\"lower80\"].values, forecast_df[\"upper80\"].values),\n    \"COV95\": coverage(y_true, forecast_df[\"lower95\"].values, forecast_df[\"upper95\"].values),\n    \"WIDTH80\": float(np.mean(forecast_df[\"upper80\"].values - forecast_df[\"lower80\"].values)),\n    \"WIDTH95\": float(np.mean(forecast_df[\"upper95\"].values - forecast_df[\"lower95\"].values)),\n}\nprint(metrics)\n\nwith open(os.path.join(OUT_DIR, \"metrics.json\"), \"w\") as f:\n    json.dump(metrics, f, indent=2)", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 9) Gr\u00e1ficas (serie, bandas 80/95, l\u00edneas train/val/test)"}, {"cell_type": "code", "metadata": {}, "source": "plt.figure(figsize=(12,5))\nplt.plot(y.index, y.values, label=\"Serie completa\", linewidth=1)\nplt.axvspan(train.index[0], train.index[-1], alpha=0.1, label=\"Train\")\nplt.axvspan(val.index[0], val.index[-1], alpha=0.1, label=\"Val\")\nplt.axvspan(test.index[0], test.index[-1], alpha=0.1, label=\"Test\")\nplt.plot(forecast_df.index, forecast_df[\"y_pred\"], label=\"Pron\u00f3stico\", linewidth=2)\nplt.fill_between(forecast_df.index, forecast_df[\"lower80\"], forecast_df[\"upper80\"], alpha=0.2, label=\"PI 80%\")\nplt.fill_between(forecast_df.index, forecast_df[\"lower95\"], forecast_df[\"upper95\"], alpha=0.15, label=\"PI 95%\")\nplt.title(f\"{TICKER} {INTERVAL} \u2014 SARIMAX (mejores hiperpar\u00e1metros)\")\nplt.legend()\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"plot_forecast.png\"), dpi=140)\nplt.show()", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 10) Diagn\u00f3sticos (residuales, QQ, ACF/PACF, Ljung-Box, normalidad)"}, {"cell_type": "code", "metadata": {}, "source": "resid = res.resid.dropna()\n\nplt.figure(figsize=(12,3))\nplt.plot(resid)\nplt.title(\"Residuales\")\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"diag_residuals.png\"), dpi=140)\nplt.show()\n\nplt.figure(figsize=(5,4))\nqqplot(resid, line=\"s\")\nplt.title(\"QQ-plot residuales\")\nplt.tight_layout()\nplt.savefig(os.path.join(OUT_DIR, \"diag_qq.png\"), dpi=140)\nplt.show()\n\nfig, ax = plt.subplots(1,1, figsize=(6,3))\nplot_acf(resid, ax=ax, lags=40)\nfig.tight_layout(); fig.savefig(os.path.join(OUT_DIR, \"diag_acf.png\"), dpi=140); plt.show()\n\nfig, ax = plt.subplots(1,1, figsize=(6,3))\nplot_pacf(resid, ax=ax, lags=40, method=\"ywm\")\nfig.tight_layout(); fig.savefig(os.path.join(OUT_DIR, \"diag_pacf.png\"), dpi=140); plt.show()\n\nlb = acorr_ljungbox(resid, lags=[10,20,30], return_df=True)\njb_stat, jb_p, _, _ = jarque_bera(resid)\nprint(\"Ljung-Box p-values:\\n\", lb[\"lb_pvalue\"])\nprint(f\"Jarque-Bera p-value: {jb_p:.4f}\")", "outputs": [], "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "## 11) Guardado final y versiones"}, {"cell_type": "code", "metadata": {}, "source": "# Ya guardamos: forecast.csv, metrics.json, plot_forecast.png, summary.txt, trials.csv, best_params.json\n# Datos limpios\ndata.to_csv(os.path.join(OUT_DIR, \"datos_limpios.csv\"))\n\n# Versions\nimport importlib, sys\ndef _v(pkg): \n    try: return importlib.import_module(pkg).__version__\n    except: return \"NA\"\nversions = {\n    \"python\": sys.version,\n    \"pandas\": _v(\"pandas\"),\n    \"numpy\": _v(\"numpy\"),\n    \"statsmodels\": _v(\"statsmodels\"),\n    \"matplotlib\": _v(\"matplotlib\"),\n    \"yfinance\": _v(\"yfinance\"),\n    \"optuna\": _v(\"optuna\"),\n    \"pandas_market_calendars\": _v(\"pandas_market_calendars\")\n}\nwith open(os.path.join(OUT_DIR, \"versions.json\"), \"w\") as f:\n    json.dump(versions, f, indent=2)\nprint(\"Versiones:\", versions)\nprint(\"Archivos guardados en:\", os.path.abspath(OUT_DIR))", "outputs": [], "execution_count": null}]}